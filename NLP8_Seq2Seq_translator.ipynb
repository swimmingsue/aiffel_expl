{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP8 Seq2Seq_translator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+VcddXsMtj1A5TUL9QfJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swimmingsue/aiffel_expl/blob/main/NLP8_Seq2Seq_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W8p7qOzXjTC",
        "outputId": "126d99c2-d858-4d5e-be50-643f73e65dc8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file_en = '/content/drive/MyDrive/GoingDeeper/Seq2Seq/korean-english-park.train.en'\n",
        "path_to_file_ko = '/content/drive/MyDrive/GoingDeeper/Seq2Seq/korean-english-park.train.ko'"
      ],
      "metadata": {
        "id": "-ZmLZaxnXrfb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BACla9AVTZ9",
        "outputId": "4965b029-dd23-42a0-8d69-21c00e9fad29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "완료!\n"
          ]
        }
      ],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "%config InlineBackend.figure_format = 'retina'\n",
        " \n",
        "import matplotlib.font_manager as fm\n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=9)\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "mpl.font_manager.findfont(font)\n",
        "\n",
        "print(\"완료!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlVVe-LhXqsK",
        "outputId": "d74c41d7-422f-40d0-d021-8dbe173805f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_to_file_en, \"r\") as f:\n",
        "    raw_en = f.read().splitlines()\n",
        "\n",
        "print(\"Data Size:\", len(raw_en))\n",
        "print(\"Example:\")\n",
        "\n",
        "for sen_en in raw_en[0:100][::20]: print(\">>\", sen_en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSsilpDYY2FC",
        "outputId": "c7b6852b-ebb1-4736-9f47-bd27b8603543"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Size: 94123\n",
            "Example:\n",
            ">> Much of personal computing is about \"can you top this?\"\n",
            ">> Amid mounting pressure on North Korea to abandon its nuclear weapons program Japanese and North Korean diplomats have resumed talks on normalizing diplomatic relations.\n",
            ">> “Guard robots are used privately and professionally to detect intruders or fire,” Karlsson said.\n",
            ">> Authorities from the Water Resources Ministry plan to begin construction next year on the controversial and hugely expensive project.\n",
            ">> Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path_to_file_ko, \"r\") as f:\n",
        "    raw_ko = f.read().splitlines()\n",
        "\n",
        "print(\"Data Size:\", len(raw_ko))\n",
        "print(\"Example:\")\n",
        "\n",
        "for sen_ko in raw_ko[0:100][::20]: print(\">>\", sen_ko)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DK5LdHnnsF4",
        "outputId": "a6bb2505-1aea-442f-b30a-b52da44ddce7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Size: 94123\n",
            "Example:\n",
            ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
            ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
            ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
            ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
            ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sen_ko"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "p5q3F00dZG54",
        "outputId": "ce580d0c-099d-4a29-b929-5bebda97b8cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sen_en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Jm83UoD2FWLx",
        "outputId": "2631b447-07e3-449c-fe97-e2322dd1fa04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Researchers also have debated whether weight-training has a big impact on the heart, since it does not give the heart and lungs the kind of workout they get from aerobic activities such as brisk walking or running for at least 20 minutes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uYbIp0fjKIh",
        "outputId": "ba62491e-fdaa-44cf-b799-54d3684b3dca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 22.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 33.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab\n"
      ],
      "metadata": {
        "id": "gNRbWfQMklVw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
        "    sentence = sentence.lower().strip()\n",
        "\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    if s_token:\n",
        "        sentence = '<start> ' + sentence\n",
        "\n",
        "    if e_token:\n",
        "        sentence += ' <end>'\n",
        "    \n",
        "    return sentence\n",
        "\n",
        "print(\"슝~\")"
      ],
      "metadata": {
        "id": "_dULXo62GVwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31ab3a9-aca2-4a4b-82ff-55ae8a2c4754"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_corpus = []\n",
        "dec_corpus = []\n",
        "\n",
        "num_examples = 10000\n",
        "\n",
        "for kor in raw_ko[:num_examples]:\n",
        "    #eng, spa = pair.split(\"\\t\")\n",
        "\n",
        "    enc_corpus.append(preprocess_sentence(kor))\n",
        "    \n",
        "\n",
        "for eng in raw_en[:num_examples]:\n",
        "    #eng, spa = pair.split(\"\\t\")\n",
        "    \n",
        "    dec_corpus.append(preprocess_sentence(eng, s_token=True, e_token=True))    \n",
        "\n",
        "print(\"Korean:\", enc_corpus[100])  \n",
        "print(\"English:\", dec_corpus[100]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkGZvqGkln-n",
        "outputId": "6bc17bf3-a9af-44fa-f76c-0a7f785e0570"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Korean: 제 차 연례 컴덱스 박람회의 개회사를 한 케이츠는 년여전 기술 산업의 거품이 붕괴된 이후에 첨단 기술에 대해 부정적인 인식이 있다고 말했다 .\n",
            "English: <start> gates , who opened the rd annual comdex trade show , said there was a negative perception of high tech following the collapse of the tech bubble about two years ago . <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(enc_corpus))\n",
        "len(dec_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ea6k2n6mhLb",
        "outputId": "9852cace-030e-4f11-c9e1-d6fe709dd86b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_corpus = dict(zip(enc_corpus, dec_corpus))\n",
        "print(len(cleaned_corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjVII4rsrRqq",
        "outputId": "6517e548-b0b0-4d79-973e-481f739dd65b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 딕셔너리 내 중복값 제거하기\n",
        "# Using dictionary comprehension\n",
        "temp = {val : key for key, val in cleaned_corpus.items()}\n",
        "cleaned_corpus = {val : key for key, val in temp.items()}"
      ],
      "metadata": {
        "id": "nDSuva8za7AQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cleaned_corpus) #11개가 줄어들었다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF8dvoWja0kE",
        "outputId": "3a268e4f-6650-4819-a5b0-f9eb2a720868"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9933"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#다시 각자 리스트로 만들어주기\n",
        "enc_corpus = list(cleaned_corpus.keys())\n",
        "dec_corpus = list(cleaned_corpus.values())"
      ],
      "metadata": {
        "id": "vkHXVaEPwsI-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, tokenizer"
      ],
      "metadata": {
        "id": "6Jv1l0B0n3td"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화하기\n",
        "enc_tensor, enc_tokenizer = tokenize(enc_corpus)\n",
        "dec_tensor, dec_tokenizer = tokenize(dec_corpus)\n",
        "\n",
        "# 훈련 데이터와 검증 데이터로 분리하기\n",
        "enc_train, enc_val, dec_train, dec_val = \\\n",
        "train_test_split(enc_tensor, dec_tensor, test_size=0.2)\n",
        "\n",
        "print(\"Korean Vocab Size:\", len(enc_tokenizer.index_word))\n",
        "print(\"English Vocab Size:\", len(dec_tokenizer.index_word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip9-0zeJoY4K",
        "outputId": "dca5aca0-cadd-4e94-fc39-8a18ea016b5a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Korean Vocab Size: 50985\n",
            "English Vocab Size: 17901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.w_dec = tf.keras.layers.Dense(units)\n",
        "        self.w_enc = tf.keras.layers.Dense(units)\n",
        "        self.w_com = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, h_enc, h_dec):\n",
        "        # h_enc shape: [batch x length x units]\n",
        "        # h_dec shape: [batch x units]\n",
        "\n",
        "        h_enc = self.w_enc(h_enc)\n",
        "        h_dec = tf.expand_dims(h_dec, 1)\n",
        "        h_dec = self.w_dec(h_dec)\n",
        "\n",
        "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
        "        \n",
        "        attn = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        context_vec = attn * h_enc\n",
        "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
        "\n",
        "        return context_vec, attn\n",
        "\n",
        "print(\"슝~\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT_8gjbQofgY",
        "outputId": "fa783905-5e5d-4527-c2f1-aba7937fef86"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(enc_units,\n",
        "                                       return_sequences=True)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.gru(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "P4q09xpRotDi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, h_dec, enc_out):\n",
        "        context_vec, attn = self.attention(enc_out, h_dec)\n",
        "\n",
        "        out = self.embedding(x)\n",
        "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
        "        \n",
        "        out, h_dec = self.gru(out)\n",
        "        out = tf.reshape(out, (-1, out.shape[2]))\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out, h_dec, attn"
      ],
      "metadata": {
        "id": "lKzKqX3OoxWs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE     = 32\n",
        "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
        "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
        "\n",
        "units         = 1024\n",
        "embedding_dim = 512\n",
        "\n",
        "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
        "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
        "\n",
        "# sample input\n",
        "sequence_len = 30\n",
        "\n",
        "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
        "sample_output = encoder(sample_enc)\n",
        "\n",
        "print ('Encoder Output:', sample_output.shape)\n",
        "\n",
        "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
        "\n",
        "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                     sample_state, sample_output)\n",
        "\n",
        "print ('Decoder Output:', sample_logits.shape)\n",
        "print ('Decoder Hidden State:', h_dec.shape)\n",
        "print ('Attention:', attn.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY-rmSpKo2rg",
        "outputId": "780129c1-a2a4-4197-99cd-5844ebd98820"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Output: (32, 30, 1024)\n",
            "Decoder Output: (32, 17902)\n",
            "Decoder Hidden State: (32, 1024)\n",
            "Attention: (32, 30, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss = loss_object(real, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "print(\"슝~\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC2y_2Gvo600",
        "outputId": "f8afbb49-3918-4a4b-8d30-1e07912148cf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
        "    bsz = src.shape[0]\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_out = encoder(src)\n",
        "        h_dec = enc_out[:, -1]\n",
        "        \n",
        "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
        "\n",
        "        for t in range(1, tgt.shape[1]):\n",
        "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
        "\n",
        "            loss += loss_function(tgt[:, t], pred)\n",
        "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
        "        \n",
        "    batch_loss = (loss / int(tgt.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return batch_loss\n",
        "\n",
        "print(\"슝~\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLxkBPPnpDY4",
        "outputId": "143500bb-ff6b-4e5b-e59a-e1a075b182c0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "슝~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm    # tqdm\n",
        "import random\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    \n",
        "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
        "    random.shuffle(idx_list)\n",
        "    t = tqdm(idx_list)    # tqdm\n",
        "\n",
        "    for (batch, idx) in enumerate(t):\n",
        "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
        "                                dec_train[idx:idx+BATCH_SIZE],\n",
        "                                encoder,\n",
        "                                decoder,\n",
        "                                optimizer,\n",
        "                                dec_tokenizer)\n",
        "    \n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
        "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm"
      ],
      "metadata": {
        "id": "x76ZB-wtqouS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c421505a-a1f9-4e62-be61-fc2051ee9959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch  1:   0%|          | 1/249 [03:42<15:18:18, 222.17s/it, Loss 2.5352]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence, encoder, decoder):\n",
        "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
        "    \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                           maxlen=40,\n",
        "                                                           padding='post')\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    enc_out = encoder(inputs)\n",
        "\n",
        "    dec_hidden = enc_out[:, -1]\n",
        "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(dec_train.shape[-1]):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = \\\n",
        "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
        "\n",
        "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention\n",
        "\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sktU5o2QpPee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "회고: 시간이 부족하여 완성하지 못하고 올리게 되어 아쉽다.  한글 영문 각각의 코퍼스를 중복제거하기 위해 딕셔너리로 만들어 주어 중복 key값이 제거 되었다. 노드에는 set 함수로 중복을 제거하라고 했지만 나는 다른 방법으로 중복값 제거 하니 11개가 줄어들었다. \n",
        "전처리 중 한글 코퍼스도 정규식에 포함되도록 수정을 해주었다. \n",
        "mecab 으로 토크나이징 하고 모델을 돌려보고 싶었는데 하지 못하고 패딩 40개 제한도 두지 않아서인지 모델이 돌아가다가 런타임 오류가 나버렸다. \n",
        "아쉽지만 이대로 제출하고 혼자 더 공부해야봐야겠다."
      ],
      "metadata": {
        "id": "N5Ggfkh6YUK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zRzjDhW1YYKk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}